{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argenis-gomez/Traductor/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9JJ7FBw84tG"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5DbIHC-F6Hf"
      },
      "source": [
        "**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpbl1pXCR0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462a9c19-33d6-4612-ee69-c8b345625b2f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR5fyJWQoZv0"
      },
      "source": [
        "!pip install -q tensorflow_text_nightly\n",
        "!pip install -q tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcvtPlp3YWu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow_text as text"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQN8jwx48_yU"
      },
      "source": [
        "# Fase 2: Pre Procesado de Datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlOT-2mlw0r"
      },
      "source": [
        "## Carga de Ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCD9jwXsLwS_"
      },
      "source": [
        "Importamos los ficheros de nuestro Google Drive personal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPT23-eZsCs6"
      },
      "source": [
        "PATH = '/content/drive/My Drive/Traductor'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Or0sLV5b8t"
      },
      "source": [
        "with open(f\"{PATH}/data/europarl-v7.es-en.en\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    europarl_en = f.read()\n",
        "with open(f\"{PATH}/data/europarl-v7.es-en.es\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    europarl_es = f.read()\n",
        "with open(f\"{PATH}/data/nonbreaking_prefix.en\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(f\"{PATH}/data/nonbreaking_prefix.es\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    non_breaking_prefix_es = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFw0D2vP_Dl"
      },
      "source": [
        "## Limpieza de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIBeGXn7LIJ"
      },
      "source": [
        "Vamos a obtener los non_breaking_prefixes como una lista de palabras limpias con un punto al final para que nos sea más fácil de utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_TeuktU40Cb"
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_es = non_breaking_prefix_es.split(\"\\n\")\n",
        "non_breaking_prefix_es = [' ' + pref + '.' for pref in non_breaking_prefix_es]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9x4mZfKMaxD"
      },
      "source": [
        "Necesitaremos cada palabra y otro símbolo que queramos mantener en minúsculas y separados por espacios para que podamos \"tokenizarlos\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dBcOZQg-LTJ"
      },
      "source": [
        "def clean_data(corpus, prefixes):\n",
        "  for prefix in prefixes:\n",
        "    corpus_en = corpus.replace(prefix, prefix + '$$$')\n",
        "  corpus = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus)\n",
        "  corpus = re.sub(r\"\\.\\$\\$\\$\", '', corpus)\n",
        "  corpus = re.sub(r\"  +\", \" \", corpus)\n",
        "  return corpus.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCf1KSbxCBC1",
        "outputId": "afcb5332-f01e-475e-bbe7-868afe7418f8"
      },
      "source": [
        "%%time\n",
        "\n",
        "print('Creando corpus...')\n",
        "corpus_en = clean_data(europarl_en, non_breaking_prefix_en)\n",
        "print('Corpus en inglés creado...')\n",
        "corpus_es = clean_data(europarl_es, non_breaking_prefix_es)\n",
        "print('Corpus en español creado...')\n",
        "print('Finalizado', end='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creando corpus...\n",
            "Corpus en inglés creado...\n",
            "Corpus en español creado...\n",
            "Finalizado\n",
            "\n",
            "CPU times: user 58.7 s, sys: 1.98 s, total: 1min\n",
            "Wall time: 1min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4dApW03lbH5"
      },
      "source": [
        "## Vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFyOMlsisz-T"
      },
      "source": [
        "VOCAB_SIZE = 2**13"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12zYcLH5o8Mv"
      },
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TysbrJM9b6gQ"
      },
      "source": [
        "bert_tokenizer_params = dict()\n",
        "reserved_tokens = [\"[PAD]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    vocab_size = VOCAB_SIZE,\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    learn_params={},\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNoMHBGfwHF3",
        "outputId": "1d273b31-ed78-4e38-858e-7964d57e8b04"
      },
      "source": [
        "%%time\n",
        "\n",
        "vocab_en_path = f'{PATH}/vocab_en.txt'\n",
        "\n",
        "if not os.path.isfile(vocab_en_path):\n",
        "  print('Creando vocabulario en inglés...')\n",
        "  vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "      tf.data.Dataset.from_tensor_slices(corpus_en).batch(1000).prefetch(tf.data.experimental.AUTOTUNE),\n",
        "      **bert_vocab_args)\n",
        "\n",
        "  write_vocab_file(vocab_en_path, vocab)\n",
        "  print('Finalizado...', end='\\n\\n')\n",
        "  \n",
        "else:\n",
        "  print('Vocabulario en inglés existente...', end='\\n\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulario en inglés existente...\n",
            "\n",
            "CPU times: user 233 µs, sys: 48 µs, total: 281 µs\n",
            "Wall time: 654 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQDV3YY-taPI",
        "outputId": "96a99cb5-5966-4afc-ce0f-058f4393c113"
      },
      "source": [
        "%%time\n",
        "\n",
        "vocab_es_path = f'{PATH}/vocab_es.txt'\n",
        "\n",
        "if not os.path.isfile(vocab_es_path):\n",
        "  print('Creando vocabulario en español...')\n",
        "  vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "      tf.data.Dataset.from_tensor_slices(corpus_es).batch(1000).prefetch(tf.data.experimental.AUTOTUNE),\n",
        "      **bert_vocab_args)\n",
        "\n",
        "  write_vocab_file(vocab_es_path, vocab)\n",
        "  print('Finalizado...', end='\\n\\n')\n",
        "\n",
        "else:\n",
        "  print('Vocabulario en español existente...', end='\\n\\n')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulario en español existente...\n",
            "\n",
            "CPU times: user 635 µs, sys: 132 µs, total: 767 µs\n",
            "Wall time: 803 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Y9v8-Tozl2"
      },
      "source": [
        "## Tokenizar el Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIqo9CtEKH93"
      },
      "source": [
        "tokenizer = tf.Module()\n",
        "tokenizer.en = text.BertTokenizer(vocab_en_path)\n",
        "tokenizer.es = text.BertTokenizer(vocab_es_path)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ichADEqFMkHO",
        "outputId": "f3b1854b-fbc5-4b83-a657-450598175a98"
      },
      "source": [
        "tokenizer_path = f\"{PATH}/ckpt/Tokenizer\"\n",
        "tf.saved_model.save(tokenizer, tokenizer_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Traductor/ckpt/Tokenizer/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N0XDAzy72F9"
      },
      "source": [
        "inputs = tokenizer.en.tokenize(corpus_en)\n",
        "outputs = tokenizer.es.tokenize(corpus_es)\n",
        "\n",
        "inputs = inputs.merge_dims(-2,-1)\n",
        "outputs = outputs.merge_dims(-2,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l9obXWt8ZRB"
      },
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkNcwu1GxIPV"
      },
      "source": [
        "inputs  = add_start_end(inputs).to_list()\n",
        "outputs = add_start_end(outputs).to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG6AlcFMpC5C"
      },
      "source": [
        "## Eliminamos las frases demasiado largas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBDYBVgA6cpo"
      },
      "source": [
        "MAX_LENGTH = 20"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CD6PLGyQWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e735069-8d5c-445e-e741-ddacfe699b99"
      },
      "source": [
        "%%time\n",
        "\n",
        "print('Eliminando frases demasiado largas...')\n",
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "print('Finalizado...', end='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eliminando frases demasiado largas...\n",
            "Finalizado...\n",
            "\n",
            "CPU times: user 5min 32s, sys: 654 ms, total: 5min 33s\n",
            "Wall time: 5min 34s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypm8h5aZQTZ1"
      },
      "source": [
        "## Creamos las entradas y las salidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FP0WPsdM8hl"
      },
      "source": [
        "A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDfLDWUONlE"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXNmfis6KG3E"
      },
      "source": [
        "pd.DataFrame(inputs).to_csv(f'{PATH}/inputs.csv', index=False)\n",
        "pd.DataFrame(outputs).to_csv(f'{PATH}/outputs.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3k3YzRdv5yk"
      },
      "source": [
        "inputs  = pd.read_csv(f'{PATH}/inputs.csv').values\n",
        "outputs = pd.read_csv(f'{PATH}/outputs.csv').values"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxMp3TOIYff"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycT0YqydRcUd"
      },
      "source": [
        "# Fase 3: Construcción del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SBoH8G4XyR9"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I"
      },
      "source": [
        "Fórmula de la Codificación Posicional:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2wc6sYlX0dr"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles # (seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcw8YIQqRhOJ"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sffhwwvX-wj"
      },
      "source": [
        "### Cálculo de la Atención"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rEoCNJURbrT"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MjtvXrfYEx7"
      },
      "source": [
        "### Sub capa de atención de encabezado múltiple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvq4I9uTX5p7"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiyuHe1OeT5N"
      },
      "source": [
        "## Codificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV0ZMH7KT_KZ"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-P92KeZih60"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DthraBEwuvl"
      },
      "source": [
        "## Descodificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZWZyFBnwy8u"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combinado con la salida del encoder \n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpzdiWHiwywF"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5sJYkjbz5DD"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqvqNjJPwyh-"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-LRThUPrso"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOdqQ5qPs8z"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hiper Parámetros\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE,\n",
        "                          vocab_size_dec=VOCAB_SIZE,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xg4Wrg1Wgl"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Goque362343"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb_32PIU5Zkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734b311d-24e1-4ce6-ca49-6ee6eeff60a6"
      },
      "source": [
        "checkpoint_path = f\"{PATH}/ckpt/Traductor\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Último checkpoint restaurado!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFK5kUx602K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41245f3-b741-4702-fdf5-fa2e4272d44b"
      },
      "source": [
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Inicio del epoch {epoch+1}\")\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(f\"\\rEpoch {epoch+1} Lote {batch} Pérdida {train_loss.result():.4f} Precisión {train_accuracy.result():.4f}\",\n",
        "                  end='')\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f\"\\nGuardando checkpoint para el epoch {epoch+1} en {ckpt_save_path}\")\n",
        "\n",
        "    print(f\"Tiempo que ha tardado el epoch: {time.time() - start:6.2f} segs\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicio del epoch 1\n",
            "Epoch 1 Lote 6650 Pérdida 0.9202 Precisión 0.4751\n",
            "Guardando checkpoint para el epoch 1 en /content/drive/My Drive/Transformer/ckpt/ckpt-16\n",
            "Tiempo que ha tardado el epoch: 6819.25 segs\n",
            "\n",
            "Inicio del epoch 2\n",
            "Epoch 2 Lote 6650 Pérdida 0.9124 Precisión 0.4763\n",
            "Guardando checkpoint para el epoch 2 en /content/drive/My Drive/Transformer/ckpt/ckpt-17\n",
            "Tiempo que ha tardado el epoch: 6877.75 segs\n",
            "\n",
            "Inicio del epoch 3\n",
            "Epoch 3 Lote 6650 Pérdida 0.9055 Precisión 0.4775\n",
            "Guardando checkpoint para el epoch 3 en /content/drive/My Drive/Transformer/ckpt/ckpt-18\n",
            "Tiempo que ha tardado el epoch: 6887.22 segs\n",
            "\n",
            "Inicio del epoch 4\n",
            "Epoch 4 Lote 6650 Pérdida 0.8985 Precisión 0.4786\n",
            "Guardando checkpoint para el epoch 4 en /content/drive/My Drive/Transformer/ckpt/ckpt-19\n",
            "Tiempo que ha tardado el epoch: 6768.95 segs\n",
            "\n",
            "Inicio del epoch 5\n",
            "Epoch 5 Lote 6650 Pérdida 0.8922 Precisión 0.4797\n",
            "Guardando checkpoint para el epoch 5 en /content/drive/My Drive/Transformer/ckpt/ckpt-20\n",
            "Tiempo que ha tardado el epoch: 6671.69 segs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oKg-2e4c_ED"
      },
      "source": [
        "transformer.save_weights(f\"{PATH}/ckpt/model_weights/model\")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzyRwDrRGdq"
      },
      "source": [
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdQxaM5BgC0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a81e98-56a3-4320-ff7a-02d1c8b25a7d"
      },
      "source": [
        "transformer.load_weights(f\"{PATH}/ckpt/model_weights/model\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc2ea571390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3hR0m_oQ2T8"
      },
      "source": [
        "def translate(sentence):\n",
        "\n",
        "  sentence  = tf.convert_to_tensor([sentence])\n",
        "  encoder_input = tokenizer.en.tokenize(sentence)\n",
        "  encoder_input = encoder_input.merge_dims(-2,-1)\n",
        "  encoder_input = add_start_end(encoder_input).to_tensor()\n",
        "\n",
        "  output = tf.convert_to_tensor([START])\n",
        "  output = tf.expand_dims(output, 0)\n",
        "\n",
        "  for i in range(MAX_LENGTH):\n",
        "    predictions = transformer(encoder_input, output, False)\n",
        "    predictions = predictions[: ,-1:, :]\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "    \n",
        "    if predicted_id == END:\n",
        "        break\n",
        "  \n",
        "  text = tokenizer.es.detokenize(output)[0].numpy()\n",
        "  text = tf.strings.reduce_join(text, separator=' ', axis=-1)\n",
        "\n",
        "  return text.numpy().decode('utf-8')[8:-6]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g3fHt86ayDsr",
        "outputId": "b7cf9f3b-8f03-4aaa-fcf9-aeab3ab8aaaa"
      },
      "source": [
        "traduction = translate(\"This is a problem we have to solve.\")\n",
        "traduction"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Este es un problema que tenemos que solucionar .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdoWKbCP7Czs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c1c0740e-494a-4e62-c75b-34ab84bfa929"
      },
      "source": [
        "traduction = translate(\"This is a really powerful tool!\")\n",
        "traduction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'¡ Es una herramienta realmente poder !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGjBEb5WFMGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df823e52-cc98-45a7-a5c8-a4c955c2c2c2"
      },
      "source": [
        "traduction = translate(\"This is an interesting course about Natural Language Processing\")\n",
        "traduction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Es un modo interesante sobre el Proceso Nandelguad .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIc0UdPGJudJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ea902f78-f6b2-4056-9d3a-a099afb546ed"
      },
      "source": [
        "traduction = translate(\"Imagine all the citizens\")\n",
        "traduction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Imaginaléndolo a todos los ciudadanos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh4SlnoyexHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fffcb08d-c072-45ac-d6a5-e66b1f4c32c8"
      },
      "source": [
        "traduction = translate(\"Alejandra is the best woman in the whole world\")\n",
        "traduction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Alejara es la mejor mujer en todo el mundo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}